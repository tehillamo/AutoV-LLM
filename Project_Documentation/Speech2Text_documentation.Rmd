---
title: "Plugin_Speech2Text: A Documentation"
author: "Tehilla Mechera-Ostrovsky"
date: "2023-08-19"
output: html_document
---

### "VoiceText" Plug-In within SjPsych:

The experiment is a vanilla JsPsych experiment. It is based on a template plugin (add the name of the plugin) and is a simple choice between two options, based on their colour (e.g., see below two circles. The participants task is to judge, which of two coloured circles is green(er)/blue(r). 



1. On the start of each trial, participants are requested to click on the "record" button. 

2. Once the button is clicked, participants are requested to engage in "think aloud" whilst making their choices. 

The function "SpeechRecognition" is used to record the voice and translate it into text. 

Note, the voice-to-text recording a new array with an additional string every time a new word is recognized. For example, if the array contains: "Hi, my name is" and the word "Nancy" is spoken/recorded, a new array is created and will contain: "Hi, my name is Nancy". To save time of data processing, only the last array is stored. 



```{js}
click_to_record.addEventListener('click',function(){    
          var speech = true;
          window.SpeechRecognition = window.webkitSpeechRecognition;
      
          const recognition = new SpeechRecognition();
          recognition.interimResults = true;
      
          recognition.addEventListener('result', e => {
              const transcript = Array.from(e.results)
                  .map(result => result[0])
                  .map(result => result.transcript)
                  .join('')
      
              //document.getElementById("convert_text").innerHTML = transcript;
              jsarray.push(transcript);
              console.log(jsarray)
                   
          });
          
          if (speech == true) {
              recognition.start();
          }

          recognition.onspeechend = () => {
            console.log("Speech has stopped being detected");
          }
          click_to_record() 
        }) 

```



3. Once the participant made their choices, they submit their choices by clicking the circle of their choice and can proceed to the next trial by clicking on the "continue" button. 

4. The experiment has 2 "difficulty" conditions: Easy and Hard. 

5. The data is stored as a .csv file and the recorded text as an array of strings.  



### Data cleaning and LLMs in Python and Huggingface using only 1 GPU on Google-colab (?)

1. For every participant, we extract the array recorded in every trial. 

2. Conversion to JSON needed?

3. Run using the "one-shot" (?) pipeline to summarize the text?

4. Use the summary/label to classify the text as a strategy 

5. Compare with classical models: DDM/SSM?




The full plugin for the calibration trials with comments:

```{js}
var jsPsychcVoiceTextCalib = (function (jspsych) {
  'use strict';

  const info = {
      name: "VoiceText_Calibration",
      parameters: {
          /**
           * The HTML string to be displayed.
           */
          stimulus: {
              type: jspsych.ParameterType.HTML_STRING,
              pretty_name: "Stimulus",
              default: undefined,
          },
          /**
           * Array containing the key(s) the subject is allowed to press to respond to the stimulus.
           */
          choices: {
              type: jspsych.ParameterType.HTML_STRING,
              pretty_name: "Choices",
              default: null,
          },
          /**
           * Any content here will be displayed below the stimulus.
           */
          prompt: {
              type: jspsych.ParameterType.HTML_STRING,
              pretty_name: "Prompt",
              default: null,
          },
          /**
           * How long to show the stimulus.
           */
          stimulus_duration: {
              type: jspsych.ParameterType.INT,
              pretty_name: "Stimulus duration",
              default: null,
          },
          /**
           * How long to show trial before it ends.
           */
          trial_duration: {
              type: jspsych.ParameterType.INT,
              pretty_name: "Trial duration",
              default: null,
          },
          /**
           * If true, trial will end when subject makes a response.
           */
          response_ends_trial: {
              type: jspsych.ParameterType.BOOL,
              pretty_name: "Response ends trial",
              default: true,
          },
      },
  };
  /**
   * **html-keyboard-response**
   *
   * jsPsych plugin for displaying a stimulus and getting a keyboard response
   *
   * @author Tehilla Ostrovsky
   * @see {@link https://www.jspsych.org/plugins/jspsych-html-keyboard-response/ html-keyboard-response plugin documentation on jspsych.org}
   */
  class VoiceText_Calibration {
      constructor(jsPsych) {
          this.jsPsych = jsPsych;
      }

   trial(display_element, trial) {

    // Definition of the HTML elements that create the experiment:
 
        // this div that holds the prompt 
        var prompt = '<div id="prompt" style = "width: 700px; text-align: center;">' + trial.prompt + '</div>'
        
        // this div that holds the stimulus (in this example, the images of the circles) 
        var Stimulus_text = '<div style = "height:150px; width:700px;" id="stimulus-1">' + trial.stimulus + '</div>'

        // this div that holds the "continue" button to terminate every trial
        var button_continue = '<input disabled type="button" id="continue-button" style = "position:relative;left:0%;margin-top:50%;" value="continue">'
        
        // this div that holds the text window for the calibration. This div includes the button (id="confirm-text-button") and the recording button.  
         var text_window = 
        '<div class="voice_to_text" style = " width: 700px;text-align: center;">' + 
        '<h2 style = color: #fff;font-size: 5px;> Voice to Text Converter </h2> ' + 
        '<div>' +
        '<input type="button" id="confirm-text-button" style = "position:absolute;margin-top:50px;margin-left:150px" value="confirm-text">'+
        '<div id="convert_text" style = "border-radius: 25px; border: 2px solid #73AD21;padding: 20px; width: 600px;height: 150px;"></div>' + 
        '</div>' +
        '<button id="click_to_record">Voice To Text</button>' + 
        '</div>'

        
        // an empty array that will hold the text produced by the recording 
        var jsarray = [];
        
        // injecting the HTML elements onto the page
        display_element.innerHTML = prompt; 
        display_element.innerHTML += Stimulus_text
        display_element.innerHTML += new_html_1;
        display_element.innerHTML += text_window;
        display_element.innerHTML += button_continue;
      
        
        // A function responsible for the the recording of the voice and the conversion into text. 
        click_to_record.addEventListener('click',function(){
            var speech = true;
            window.SpeechRecognition = window.webkitSpeechRecognition;
        
            const recognition = new SpeechRecognition();
            recognition.interimResults = true;
        
            // creating the text from the speech and converting it into an array called "transcript" 
            recognition.addEventListener('result', e => {
                const transcript = Array.from(e.results)
                    .map(result => result[0])
                    .map(result => result.transcript)
                    .join('')
                
                // injecting the text into the text-wondow for participants to see and confirm
                document.getElementById("convert_text").innerHTML = transcript;
                console.log(transcript);
            });
            
            if (speech == true) {
                recognition.start();
            }
            // "onclick" function applied on the "confirm text" button. It comapres between the text recorded 
            // by the participants (which they see inside the text window) and the prompt 
            document.getElementById("confirm-text-button").onclick = function(){
                if (document.getElementById("convert_text").innerHTML == trial.prompt) {
                    alert("match!")
                    document.getElementById("continue-button").disabled = false
                } else {
                    alert("mismatch!")
                    document.getElementById("continue-button").disabled = false
                }
            }

        })

        // saving the transcript inside the object "jsText" for later saving 
        var jsText = jsarray

      
        // start time
        var start_time = performance.now();
        // add event listeners to buttons
        
            display_element
                .querySelector("#continue-button")
                .addEventListener("click", (e) => {
                var btn_el = e.currentTarget;
                var choice = btn_el.getAttribute("continue-button"); 

                after_response(choice);
            })
        

          // store response
          var response = {
            rt: null,
            button: null,
            text: null, 
        };


        // function to end trial when it is time
        const end_trial = () => {
            // kill any remaining setTimeout handlers
            this.jsPsych.pluginAPI.clearAllTimeouts();
            // gather the data to store for the trial
            var trial_data = {
                rt: response.rt,
                stimulus: trial.stimulus,
                // here the text is saved into the data file as the column "text", 
                // because every time a word/a couple of words is/are spoken, the jsText is saving it as a separated string, which creates unneccesary repetition. 
                // The final string is extracted as the last part of the text string, which contains all the words spoken (hence, the last element of the full-repeated string)  
                text: jsText.slice(-1) 
            };
            // clear the display
            display_element.innerHTML = "";
            // move on to the next trial
            this.jsPsych.finishTrial(trial_data);
        };
        // function to handle responses by the subject
        function after_response(choice) {
            // measure rt
            var end_time = performance.now();
            var rt = Math.round(end_time - start_time);
            response.button = parseInt(choice);
            response.rt = rt;
            // after a valid response, the stimulus will have the CSS class 'responded'
            // which can be used to provide visual feedback that a response was recorded
                 //display_element.querySelector("continue-button").className += //these two lines were commented out 
                 //  " responded";//these two lines were commented out 
            // disable all the buttons after a response
            var btns = document.querySelectorAll("confirm-button");
            for (var i = 0; i < btns.length; i++) {
                //btns[i].removeEventListener('click');
                btns[i].setAttribute("disabled", "disabled");
            }
            if (trial.response_ends_trial) {
                end_trial();
            }
        }
        // hide image if timing is set
        if (trial.stimulus_duration !== null) {
            this.jsPsych.pluginAPI.setTimeout(() => {
                display_element.querySelector("jspsych-voice-2-text").style.visibility = "hidden";
            }, trial.stimulus_duration);
        }
        // end trial if time limit is set
        if (trial.trial_duration !== null) {
            this.jsPsych.pluginAPI.setTimeout(end_trial, trial.trial_duration);
        }
    } //closing brackets for trial

    simulate(trial, simulation_mode, simulation_options, load_callback) {
        if (simulation_mode == "data-only") {
            load_callback();
            this.simulate_data_only(trial, simulation_options);
        }
        if (simulation_mode == "visual") {
            this.simulate_visual(trial, simulation_options, load_callback);
        }
    }
    create_simulation_data(trial, simulation_options) {
        const default_data = {
            stimulus: trial.stimulus,
            rt: this.jsPsych.randomization.sampleExGaussian(500, 50, 1 / 150, true),
            response: this.jsPsych.randomization.randomInt(0, trial.choices.length - 1),
        };
        const data = this.jsPsych.pluginAPI.mergeSimulationData(default_data, simulation_options);
        this.jsPsych.pluginAPI.ensureSimulationDataConsistency(trial, data);
        return data;
    }
    simulate_data_only(trial, simulation_options) {
        const data = this.create_simulation_data(trial, simulation_options);
        this.jsPsych.finishTrial(data);
    }
    simulate_visual(trial, simulation_options, load_callback) {
        const data = this.create_simulation_data(trial, simulation_options);
        const display_element = this.jsPsych.getDisplayElement();
        this.trial(display_element, trial);
        load_callback();
        if (data.rt !== null) {
            this.jsPsych.pluginAPI.clickTarget(display_element.querySelector(`div[data-choice="${data.response}"] button`), data.rt);
        }
    }

}  
    

VoiceText_Calibration.info = info;

return VoiceText_Calibration;

})(jsPsychModule);
```



